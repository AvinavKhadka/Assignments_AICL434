{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2dfc6f0",
   "metadata": {},
   "source": [
    "# Assignment 3.3\n",
    "\n",
    "This notebook contains the solution and outputs for **Assignment 3.3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e992b",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37892a3d-88e1-45fc-a30c-7f4ec7bb72f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:23:00.916687Z",
     "iopub.status.busy": "2025-06-26T15:23:00.916422Z",
     "iopub.status.idle": "2025-06-26T15:23:03.153364Z",
     "shell.execute_reply": "2025-06-26T15:23:03.152689Z",
     "shell.execute_reply.started": "2025-06-26T15:23:00.916667Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tqdm\n",
    "from bert_from_scratch import BertForSequenceClassification as MyBertForSequenceClassification\n",
    "from lora_from_scratch import add_lora_layers, freeze_model, merge_lora_layers, unfreeze_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccae90f-9179-4b9d-83cf-315889cfe49f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:23:03.736078Z",
     "iopub.status.busy": "2025-06-26T15:23:03.735225Z",
     "iopub.status.idle": "2025-06-26T15:23:36.855876Z",
     "shell.execute_reply": "2025-06-26T15:23:36.855368Z",
     "shell.execute_reply.started": "2025-06-26T15:23:03.736049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b695c052e0ae4be9bea9fe8e206a17a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb0f0bc84d24710993ac4ef716388d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14027 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf8e92206a249c2a2ecfcce4e7c37ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    text = [item['text'] for item in batch]\n",
    "    labels = torch.stack([torch.tensor(item['label']) for item in batch])\n",
    "    return {'text': text, 'label': labels}\n",
    "\n",
    "df = pd.read_csv(\"../data/train-sample.csv\")\n",
    "string_to_int = {'open': 0, 'not a real question': 1, 'off topic': 1, 'not constructive': 1, 'too localized': 1}\n",
    "df['OpenStatusInt'] = df['OpenStatus'].map(string_to_int)\n",
    "df['TitleConcatWithBody'] = df.apply(lambda x: x.Title + \" \" + x.BodyMarkdown, axis=1)\n",
    "data_dict = {'text': df.TitleConcatWithBody.tolist(), 'label': df.OpenStatusInt.tolist()}\n",
    "dataset_stackoverflow = Dataset.from_dict(data_dict)\n",
    "n_samples = len(dataset_stackoverflow)\n",
    "split_idx1 = int(n_samples * 0.8)\n",
    "split_idx2 = int(n_samples * 0.9)\n",
    "shuffled_dataset = dataset_stackoverflow.shuffle(seed=42)\n",
    "train_dataset = shuffled_dataset.select(range(split_idx1))\n",
    "val_dataset = shuffled_dataset.select(range(split_idx1, split_idx2))\n",
    "test_dataset = shuffled_dataset.select(range(split_idx2, n_samples))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcafef1-ca46-4a64-8563-b25de7403bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:23:36.569496Z",
     "iopub.status.busy": "2025-06-26T15:23:36.569176Z",
     "iopub.status.idle": "2025-06-26T15:23:36.589597Z",
     "shell.execute_reply": "2025-06-26T15:23:36.588825Z",
     "shell.execute_reply.started": "2025-06-26T15:23:36.569473Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertTrainer:\n",
    "    def __init__(self, model, tokenizer, train_dataloader, eval_dataloader=None, epochs=1, lr=5e-4, output_dir='./', output_filename='model_state_dict.pt', save=False):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.output_dir = output_dir\n",
    "        self.output_filename = output_filename\n",
    "        self.save = save\n",
    "        self.eval_loss = float('inf')\n",
    "        self.epochs = epochs\n",
    "        self.epoch_best_model = 0\n",
    "\n",
    "    def train(self, evaluate=False):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.iteration(epoch, self.train_dataloader)\n",
    "            if evaluate and self.eval_dataloader is not None:\n",
    "                self.iteration(epoch, self.eval_dataloader, train=False)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.iteration(0, self.eval_dataloader, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        loss_accumulated = 0.\n",
    "        correct_accumulated = 0\n",
    "        samples_accumulated = 0\n",
    "        preds_all = []\n",
    "        labels_all = []\n",
    "        self.model.train() if train else self.model.eval()\n",
    "        mode = \"train\" if train else \"eval\"\n",
    "        batch_iter = tqdm.tqdm(enumerate(data_loader), desc=f\"EP ({mode}) {epoch}\", total=len(data_loader), bar_format=\"{l_bar}{r_bar}\")\n",
    "        for i, batch in batch_iter:\n",
    "            batch_t = self.tokenizer(batch['text'], padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
    "            batch_t = {key: value.to(self.device) for key, value in batch_t.items()}\n",
    "            batch_t[\"input_labels\"] = batch[\"label\"].to(self.device)\n",
    "            logits = self.model(input_ids=batch_t[\"input_ids\"], token_type_ids=batch_t[\"token_type_ids\"], attention_mask=batch_t[\"attention_mask\"])\n",
    "            loss = self.loss_fn(logits, batch_t[\"input_labels\"])\n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct = preds.eq(batch_t[\"input_labels\"]).sum().item()\n",
    "            loss_accumulated += loss.item()\n",
    "            correct_accumulated += correct\n",
    "            samples_accumulated += len(batch_t[\"input_labels\"])\n",
    "            preds_all.append(preds.detach())\n",
    "            labels_all.append(batch_t['input_labels'].detach())\n",
    "        preds_all = torch.cat(preds_all, dim=0).cpu()\n",
    "        labels_all = torch.cat(labels_all, dim=0).cpu()\n",
    "        accuracy = accuracy_score(labels_all, preds_all)\n",
    "        precision = precision_score(labels_all, preds_all, average='macro')\n",
    "        recall = recall_score(labels_all, preds_all, average='macro')\n",
    "        f1 = f1_score(labels_all, preds_all, average='macro')\n",
    "        avg_loss_epoch = loss_accumulated / len(data_loader)\n",
    "        print(f\"samples={samples_accumulated}, correct={correct_accumulated}, acc={round(accuracy, 4)}, recall={round(recall, 4)}, prec={round(precision,4)}, f1={round(f1, 4)}, loss={round(avg_loss_epoch, 4)}\")\n",
    "        if self.save and not train and avg_loss_epoch < self.eval_loss:\n",
    "            dir_path = Path(self.output_dir)\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = dir_path / f\"{self.output_filename}_epoch_{epoch}.pt\"\n",
    "            if epoch > 0:\n",
    "                file_path_best_model = dir_path / f\"{self.output_filename}_epoch_{self.epoch_best_model}.pt\"\n",
    "                !rm -f $file_path_best_model\n",
    "            torch.save({'model_state_dict': self.model.state_dict(), 'optimizer_state_dict': self.optimizer.state_dict()}, file_path)\n",
    "            self.eval_loss = avg_loss_epoch\n",
    "            self.epoch_best_model = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f59a3be-870e-47f1-a7ba-41095674f75a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:23:36.221555Z",
     "iopub.status.busy": "2025-06-26T15:23:36.220927Z",
     "iopub.status.idle": "2025-06-26T15:23:53.137932Z",
     "shell.execute_reply": "2025-06-26T15:23:53.137327Z",
     "shell.execute_reply.started": "2025-06-26T15:23:36.221525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c186442942348cdac8d0dc31fdddacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fe46890cad4b83be80e85a7319b343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbf4f585dd34c82b412626217ed846f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c694280461a46c38c8c4fac52173ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_base = MyBertForSequenceClassification.from_pretrained(\n",
    "    model_type='bert-base-uncased',\n",
    "    config_args={\"vocab_size\": 30522, \"n_classes\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4845b80-9006-4b56-9989-86a6d533b5c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:23:53.595516Z",
     "iopub.status.busy": "2025-06-26T15:23:53.594460Z",
     "iopub.status.idle": "2025-06-26T19:29:53.922869Z",
     "shell.execute_reply": "2025-06-26T19:29:53.922170Z",
     "shell.execute_reply.started": "2025-06-26T15:23:53.595476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 0: 100%|| 3507/3507 [48:24<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=85975, acc=0.7661, recall=0.7662, prec=0.7663, f1=0.7661, loss=0.4903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=11013, acc=0.7851, recall=0.7853, prec=0.7856, f1=0.7851, loss=0.4643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 1: 100%|| 3507/3507 [48:27<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=89764, acc=0.7999, recall=0.7999, prec=0.7999, f1=0.7999, loss=0.4392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 1: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=11034, acc=0.7866, recall=0.7869, prec=0.7877, f1=0.7865, loss=0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 2: 100%|| 3507/3507 [48:29<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=92032, acc=0.8201, recall=0.8201, prec=0.8201, f1=0.8201, loss=0.4056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 2: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=11081, acc=0.79, recall=0.7901, prec=0.7902, f1=0.79, loss=0.4624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 3: 100%|| 3507/3507 [48:31<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=94453, acc=0.8417, recall=0.8417, prec=0.8417, f1=0.8417, loss=0.3671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 3: 100%|| 439/439 [02:26<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=10886, acc=0.7761, recall=0.7767, prec=0.7816, f1=0.7752, loss=0.4853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 4: 100%|| 3507/3507 [48:34<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=97186, acc=0.8661, recall=0.8661, prec=0.8661, f1=0.8661, loss=0.3244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 4: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=10998, acc=0.7841, recall=0.7842, prec=0.7842, f1=0.7841, loss=0.5039\n"
     ]
    }
   ],
   "source": [
    "trainer_bert_base = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-6,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned',\n",
    "    output_filename='bert_base',\n",
    "    save=True\n",
    ")\n",
    "trainer_bert_base.train(evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf271c4-ab71-49e5-a2a0-c560689f4e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:29:53.369318Z",
     "iopub.status.busy": "2025-06-26T19:29:53.368732Z",
     "iopub.status.idle": "2025-06-26T19:32:23.678915Z",
     "shell.execute_reply": "2025-06-26T19:32:23.678066Z",
     "shell.execute_reply.started": "2025-06-26T19:29:53.369294Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14028, correct=11022, acc=0.7857, recall=0.7855, prec=0.7862, f1=0.7855, loss=0.4594\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('../models/bert_base_fine_tuned/bert_base_epoch_1.pt')\n",
    "bert_base.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "trainer_bert_base = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-6,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned',\n",
    "    output_filename='bert_base',\n",
    "    save=False\n",
    ")\n",
    "trainer_bert_base.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b1890c-40ad-4e05-af6c-bf213734ddf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:32:23.125052Z",
     "iopub.status.busy": "2025-06-26T19:32:23.124454Z",
     "iopub.status.idle": "2025-06-26T19:32:25.172040Z",
     "shell.execute_reply": "2025-06-26T19:32:25.171438Z",
     "shell.execute_reply.started": "2025-06-26T19:32:23.125018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_base = MyBertForSequenceClassification.from_pretrained(\n",
    "    model_type='bert-base-uncased',\n",
    "    config_args={\"vocab_size\": 30522, \"n_classes\": 2}\n",
    ")\n",
    "add_lora_layers(bert_base, r=8, lora_alpha=16)\n",
    "freeze_model(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9e380c-abc9-4303-8e61-45aec24ac4e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:32:25.706451Z",
     "iopub.status.busy": "2025-06-26T19:32:25.705940Z",
     "iopub.status.idle": "2025-06-26T19:32:25.711337Z",
     "shell.execute_reply": "2025-06-26T19:32:25.710699Z",
     "shell.execute_reply.started": "2025-06-26T19:32:25.706430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 109778690\n",
      "Trainable parameters: 296450\n",
      "Percentage trainable: 0.27%\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "n_trainable_params = 0\n",
    "for n, p in bert_base.named_parameters():\n",
    "    n_params += p.numel()\n",
    "    if p.requires_grad:\n",
    "        n_trainable_params += p.numel()\n",
    "print(f\"Total parameters: {n_params}\")\n",
    "print(f\"Trainable parameters: {n_trainable_params}\")\n",
    "print(f\"Percentage trainable: {round(n_trainable_params / n_params * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20b90b17-792a-4637-b3c4-a009eac57551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:32:25.840874Z",
     "iopub.status.busy": "2025-06-26T19:32:25.840080Z",
     "iopub.status.idle": "2025-06-26T22:44:44.433645Z",
     "shell.execute_reply": "2025-06-26T22:44:44.433047Z",
     "shell.execute_reply.started": "2025-06-26T19:32:25.840839Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 0: 100%|| 3507/3507 [38:32<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=86105, acc=0.7673, recall=0.7673, prec=0.7674, f1=0.7673, loss=0.4879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:28<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=10730, acc=0.765, recall=0.764, prec=0.776, f1=0.7621, loss=0.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 1: 100%|| 3507/3507 [38:33<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=89196, acc=0.7949, recall=0.7949, prec=0.7949, f1=0.7949, loss=0.4482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 1: 100%|| 439/439 [02:29<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=10908, acc=0.7776, recall=0.7777, prec=0.7777, f1=0.7776, loss=0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 2: 100%|| 3507/3507 [38:33<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=90999, acc=0.8109, recall=0.8109, prec=0.8109, f1=0.8109, loss=0.4214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 2: 100%|| 439/439 [02:28<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=10986, acc=0.7832, recall=0.7834, prec=0.7839, f1=0.7831, loss=0.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 3: 100%|| 3507/3507 [38:35<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=92469, acc=0.824, recall=0.824, prec=0.824, f1=0.824, loss=0.3965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 3: 100%|| 439/439 [02:29<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=10937, acc=0.7797, recall=0.7799, prec=0.7802, f1=0.7797, loss=0.4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 4: 100%|| 3507/3507 [38:34<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217, correct=94023, acc=0.8379, recall=0.8379, prec=0.8379, f1=0.8379, loss=0.3718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 4: 100%|| 439/439 [02:28<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027, correct=10862, acc=0.7744, recall=0.7745, prec=0.7747, f1=0.7743, loss=0.5011\n"
     ]
    }
   ],
   "source": [
    "trainer_bert_base_lora = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-4,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_lora_r8',\n",
    "    output_filename='bert_base_lora_r8',\n",
    "    save=True\n",
    ")\n",
    "trainer_bert_base_lora.train(evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf3d05f-49a1-44e6-ad01-d0299180cfb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T22:44:44.001180Z",
     "iopub.status.busy": "2025-06-26T22:44:44.000545Z",
     "iopub.status.idle": "2025-06-26T22:44:44.004613Z",
     "shell.execute_reply": "2025-06-26T22:44:44.003903Z",
     "shell.execute_reply.started": "2025-06-26T22:44:44.001149Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../models/bert_base_fine_tuned_lora_r8/bert_base_lora_r8_epoch_1.pt\")\n",
    "bert_base.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "merge_lora_layers(bert_base)\n",
    "unfreeze_model(bert_base)\n",
    "dir_path = Path(\"../models/bert_base_fine_tuned_lora_r8/merged\")\n",
    "dir_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = dir_path / \"bert_base_lora_r8_epoch_1_merged.pt\"\n",
    "torch.save({\"model_state_dict\": bert_base.state_dict()}, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc973148-5ec8-4576-ab18-d5c2a7a59da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T22:44:44.460915Z",
     "iopub.status.busy": "2025-06-26T22:44:44.460140Z",
     "iopub.status.idle": "2025-06-26T22:47:07.430295Z",
     "shell.execute_reply": "2025-06-26T22:47:07.429834Z",
     "shell.execute_reply.started": "2025-06-26T22:44:44.460890Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:22<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14028, correct=10985, acc=0.7831, recall=0.8204, prec=0.7395, f1=0.7778, loss=0.4643\n"
     ]
    }
   ],
   "source": [
    "trainer_bert_base_lora_r8 = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-6,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_lora_r8',\n",
    "    output_filename='bert_base_lora_r8',\n",
    "    save=False\n",
    ")\n",
    "trainer_bert_base_lora_r8.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c912624",
   "metadata": {},
   "source": [
    "# Assingment 3.1\n",
    "\n",
    "This notebook contains the solution and outputs for **Assingment 3.1**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0238f50",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'web_content' already exists. Reusing it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components initialized successfully.\n",
      "Crawling website...\n",
      "Crawled https://en.wikipedia.org/wiki/Artificial_intelligence: 86139 characters extracted\n",
      "Crawled https://en.wikipedia.org/wiki/Artificial_intelligence#bodyContent: 86139 characters extracted\n",
      "Crawled https://en.wikipedia.org/wiki/Artificial_intelligence#Goals: 86139 characters extracted\n",
      "Crawled https://en.wikipedia.org/wiki/Artificial_intelligence#Reasoning_and_problem-solving: 86139 characters extracted\n",
      "Crawled https://en.wikipedia.org/wiki/Artificial_intelligence#Knowledge_representation: 86139 characters extracted\n",
      "Crawled 5 pages in 9.19 seconds\n",
      "Storing documents...\n",
      "Stored document 0 from https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "Stored document 1 from https://en.wikipedia.org/wiki/Artificial_intelligence#bodyContent\n",
      "Stored document 2 from https://en.wikipedia.org/wiki/Artificial_intelligence#Goals\n",
      "Stored document 3 from https://en.wikipedia.org/wiki/Artificial_intelligence#Reasoning_and_problem-solving\n",
      "Stored document 4 from https://en.wikipedia.org/wiki/Artificial_intelligence#Knowledge_representation\n",
      "Answering questions...\n",
      "Context length for question 'What is artificial intelligence?': 172279 characters\n",
      "Context length for question 'What are the applications of AI?': 172279 characters\n",
      "Context length for question 'Who invented the term AI?': 172279 characters\n",
      "Question: What is artificial intelligence?\n",
      "Answer: storytelling devices\n",
      "Context: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...\n",
      "--------------------------------------------------\n",
      "Question: What are the applications of AI?\n",
      "Answer: reasoning, planning, learning, perception, and robotics\n",
      "Context: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...\n",
      "--------------------------------------------------\n",
      "Question: Who invented the term AI?\n",
      "Answer: Geoffrey Hinton\n",
      "Context: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...\n",
      "--------------------------------------------------\n",
      "Evaluation: Manual inspection required for real datasets like Natural Questions\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Initialize components\n",
    "try:\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    chroma_client = chromadb.Client()\n",
    "    # Check if collection exists, delete and recreate or reuse\n",
    "    collection_name = \"web_content\"\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "        print(f\"Collection '{collection_name}' already exists. Reusing it.\")\n",
    "    except:\n",
    "        print(f\"Creating new collection '{collection_name}'.\")\n",
    "        chroma_client.delete_collection(collection_name)  # Delete if exists\n",
    "        collection = chroma_client.create_collection(collection_name)\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=-1)  # Explicitly use CPU\n",
    "    print(\"Components initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing components: {e}\")\n",
    "    raise\n",
    "\n",
    "# Function to crawl a website and extract text\n",
    "def crawl_website(url, max_pages=5):\n",
    "    visited = set()\n",
    "    to_visit = [url]\n",
    "    documents = []\n",
    "    \n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        try:\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Added to avoid rate-limiting\n",
    "            response = requests.get(current_url, timeout=5, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Skipping {current_url}: Status code {response.status_code}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract text from paragraphs\n",
    "            text = ' '.join([p.get_text().strip() for p in soup.find_all('p') if p.get_text().strip()])\n",
    "            if text:\n",
    "                documents.append({\"url\": current_url, \"text\": text})\n",
    "                print(f\"Crawled {current_url}: {len(text)} characters extracted\")\n",
    "            else:\n",
    "                print(f\"No text extracted from {current_url}\")\n",
    "            \n",
    "            # Find links to follow\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                next_url = urljoin(current_url, link['href'])\n",
    "                if next_url.startswith(url) and next_url not in visited:\n",
    "                    to_visit.append(next_url)\n",
    "            \n",
    "            visited.add(current_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {current_url}: {e}\")\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"Warning: No documents were crawled.\")\n",
    "    return documents\n",
    "\n",
    "# Function to store documents in vector database\n",
    "def store_documents(documents):\n",
    "    if not documents:\n",
    "        print(\"No documents to store.\")\n",
    "        return\n",
    "    for i, doc in enumerate(documents):\n",
    "        try:\n",
    "            embedding = embedder.encode(doc[\"text\"]).tolist()\n",
    "            collection.add(\n",
    "                documents=[doc[\"text\"]],\n",
    "                embeddings=[embedding],\n",
    "                metadatas=[{\"url\": doc[\"url\"]}],\n",
    "                ids=[f\"doc_{i}\"]\n",
    "            )\n",
    "            print(f\"Stored document {i} from {doc['url']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing document {i} from {doc['url']}: {e}\")\n",
    "\n",
    "# Function to retrieve relevant documents and answer question\n",
    "def answer_question(question):\n",
    "    try:\n",
    "        query_embedding = embedder.encode(question).tolist()\n",
    "        results = collection.query(query_embeddings=[query_embedding], n_results=2)\n",
    "        documents = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "        \n",
    "        if not documents:\n",
    "            print(f\"No documents retrieved for question: {question}\")\n",
    "            return \"No relevant information found.\", \"\"\n",
    "        \n",
    "        context = \" \".join(documents)\n",
    "        if not context.strip():\n",
    "            print(f\"Empty context for question: {question}\")\n",
    "            return \"No relevant information found.\", \"\"\n",
    "        \n",
    "        print(f\"Context length for question '{question}': {len(context)} characters\")\n",
    "        answer = qa_pipeline(question=question, context=context)\n",
    "        return answer[\"answer\"], context\n",
    "    except Exception as e:\n",
    "        print(f\"Error answering question '{question}': {e}\")\n",
    "        return \"Error processing question.\", \"\"\n",
    "\n",
    "# Main pipeline\n",
    "def rag_pipeline(start_url, questions):\n",
    "    print(\"Crawling website...\")\n",
    "    start_time = time.time()\n",
    "    documents = crawl_website(start_url)\n",
    "    print(f\"Crawled {len(documents)} pages in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"Storing documents...\")\n",
    "    store_documents(documents)\n",
    "    \n",
    "    print(\"Answering questions...\")\n",
    "    results = []\n",
    "    for question in questions:\n",
    "        answer, context = answer_question(question)\n",
    "        results.append({\"question\": question, \"answer\": answer, \"context\": context})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Example website and questions\n",
    "        start_url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "        questions = [\n",
    "            \"What is artificial intelligence?\",\n",
    "            \"What are the applications of AI?\",\n",
    "            \"Who invented the term AI?\"\n",
    "        ]\n",
    "        \n",
    "        results = rag_pipeline(start_url, questions)\n",
    "        for result in results:\n",
    "            print(f\"Question: {result['question']}\")\n",
    "            print(f\"Answer: {result['answer']}\")\n",
    "            print(f\"Context: {result['context'][:200]}...\" if result['context'] else \"Context: None\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        # Evaluation (simplified)\n",
    "        print(\"Evaluation: Manual inspection required for real datasets like Natural Questions\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running RAG pipeline: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

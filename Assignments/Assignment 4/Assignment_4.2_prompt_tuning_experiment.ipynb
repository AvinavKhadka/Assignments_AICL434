{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d01b4519",
      "metadata": {},
      "source": [
        "# Assignment 4.2: Prompt Tuning Experiment\n",
        "\n",
        "In this experiment, we compare prompt tuning styles for **sentiment analysis** using the `Flan-T5` model. We:\n",
        "- Start with a basic direct prompt\n",
        "- Apply contextual enhancement\n",
        "- Use a pattern-based rephrased prompt\n",
        "\n",
        "We evaluate results using EM and F1 metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6c4c75c",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50effac6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_metric\n",
        "import torch\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d86ba31",
      "metadata": {},
      "source": [
        "## Task Definition\n",
        "\n",
        "Given a movie review, we classify its sentiment into one of the three classes: `positive`, `neutral`, or `negative`.\n",
        "\n",
        "**Input sentence:**\n",
        "> \"The movie was surprisingly good and kept me engaged till the end.\"\n",
        "\n",
        "**Expected label:** `positive`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc117490",
      "metadata": {},
      "source": [
        "### 1. Direct Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e3cb71a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive"
          ]
        }
      ],
      "source": [
        "prompt_direct = \"Sentiment of the following sentence: The movie was surprisingly good and kept me engaged till the end.\"\n",
        "inputs = tokenizer(prompt_direct, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "direct_pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(direct_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e260f7",
      "metadata": {},
      "source": [
        "### 2. Contextual Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "71454e08",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive"
          ]
        }
      ],
      "source": [
        "prompt_contextual = \"Analyze sentiment. Sentence: 'The movie was surprisingly good and kept me engaged till the end.'\\nOptions: positive, neutral, negative. Answer:\"\n",
        "inputs = tokenizer(prompt_contextual, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "contextual_pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(contextual_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a996e595",
      "metadata": {},
      "source": [
        "### 3. Pattern-based Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a56c9ba0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive"
          ]
        }
      ],
      "source": [
        "prompt_pattern = \"Text: 'The movie was surprisingly good and kept me engaged till the end.'\\nSentiment classification (positive / neutral / negative):\"\n",
        "inputs = tokenizer(prompt_pattern, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "pattern_pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(pattern_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96d08c8e",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This experiment shows that prompt tuning using structured and contextual phrasing improves interpretability and reliability. For simple cases, all prompt styles yielded correct predictions. However, advanced prompts are expected to generalize better."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
